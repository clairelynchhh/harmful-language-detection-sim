{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef2122e4-93f6-4779-b506-5c5b368048fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Run once\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "!pip install --quiet transformers datasets accelerate sentencepiece --upgrade\n",
    "!pip install --quiet tqdm scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd5996d2-1a5b-48d9-afee-091112c63549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "random seed: 1234\n"
     ]
    }
   ],
   "source": [
    "# CSC396 - Intro to Deep Learning w/ NLP\n",
    "# Claire Lynch & Jose Santiago Campa Morales\n",
    "# Final Project\n",
    "# Hate Speech and Offensive Language Classifier\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# enable tqdm in pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "hf_logging.set_verbosity_error()  # reduce HF noise\n",
    "\n",
    "# set to True to use the gpu (if there is one available)\n",
    "use_gpu = True\n",
    "# select device\n",
    "device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device.type}')\n",
    "\n",
    "# Model config:\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 3\n",
    "LR = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "OUTPUT_DIR = Path(\"checkpoints/roberta_hateclf\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SEED = 42\n",
    "\n",
    "# random seed\n",
    "seed = 1234\n",
    "\n",
    "# set random seed\n",
    "if seed is not None:\n",
    "    print(f'random seed: {seed}')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3583144c-4092-4bb5-9483-1cc2b08d16d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning text + emojis\n",
    "\n",
    "import re\n",
    "import html\n",
    "import emoji\n",
    "\n",
    "def normalize_repeated_chars(word):\n",
    "    # Replace 3+ repeated characters with exactly 2 (e.g. \"soooo\" â†’ \"soo\")\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', word)\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    # 1. Lowercase\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    # 2. Remove backslashes\n",
    "    tweet = tweet.replace('\\\\', ' ')\n",
    "\n",
    "    # 3. Decode HTML entities\n",
    "    tweet = html.unescape(tweet)\n",
    "\n",
    "    # 4. Remove URLs\n",
    "    tweet = re.sub(r'http\\S+|www\\S+', '', tweet)\n",
    "\n",
    "    # 5. Tokenize using the HF tokenizer (preserves subwords)\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    clean_tokens = []\n",
    "    for t in tokens:\n",
    "        # 6. Remove leading @\n",
    "        if t.startswith('@'):\n",
    "            t = t[1:]\n",
    "\n",
    "        # 7. Remove leading #\n",
    "        if t.startswith('#'):\n",
    "            t = t[1:]\n",
    "\n",
    "        # 8. Convert emojis into text\n",
    "        if emoji.is_emoji(t):\n",
    "            t = emoji.demojize(t)\n",
    "\n",
    "        # 9. Remove unwanted characters\n",
    "        t = re.sub(r'[^a-zA-Z0-9:]', '', t)\n",
    "\n",
    "        # 10. Normalize repeated letters\n",
    "        t = normalize_repeated_chars(t)\n",
    "\n",
    "        # 11. Keep non-empty tokens\n",
    "        if t:\n",
    "            clean_tokens.append(t)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e99f938d-061f-404b-a8f7-d00627a2f817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns found: ['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither', 'class', 'tweet']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad78723d7934d7ba32760331c5a7d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24783 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: data/labeled_data.csv\n",
      "Label distribution (counts):\n",
      "label\n",
      "1    19190\n",
      "2     4163\n",
      "0     1430\n",
      "Name: count, dtype: int64\n",
      "   class                                               text  \\\n",
      "0      2  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
      "1      1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
      "2      1  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [r, t, may, as, ol, ove, ly, :, as, a, woman, ...   \n",
      "1  [r, t, m, le, ew, 17, :, boy, d, ats, cold, ty...   \n",
      "2  [r, t, ur, kind, of, brand, d, aw, g, r, t, 80...   \n",
      "\n",
      "                                          clean_text          label_name  \\\n",
      "0  r t may as ol ove ly : as a woman you shouldn ...             neither   \n",
      "1  r t m le ew 17 : boy d ats cold ty ga d wn bad...  offensive_language   \n",
      "2  r t ur kind of brand d aw g r t 80 sb aby 4 li...  offensive_language   \n",
      "\n",
      "   label  \n",
      "0      2  \n",
      "1      1  \n",
      "2      1  \n",
      "Split sizes: Train: 17347, Val: 2479, Test: 4957\n",
      "\n",
      "Train label distribution:\n",
      "label\n",
      "1    13432\n",
      "2     2914\n",
      "0     1001\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read dataset, clean, map labels, split\n",
    "def read_data_df(filename, test_size=0.2, val_size=0.1, random_state=seed):\n",
    "    \"\"\"\n",
    "    Read CSV and return train_df, val_df, test_df (plus extracted lists).\n",
    "    Expects original columns including 'class' and 'tweet'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    print(\"Columns found:\", df.columns.tolist())   # DEBUG\n",
    "\n",
    "    # Your dataset ALWAYS has these columns:\n",
    "    # ['count', 'hate_speech', 'offensive_language', 'neither', 'class', 'tweet']\n",
    "\n",
    "    # Keep only tweet + class\n",
    "    df = df[['class', 'tweet']].copy()\n",
    "\n",
    "    # Rename tweet -> text (CRUCIAL)\n",
    "    df = df.rename(columns={'tweet': 'text'})\n",
    "\n",
    "    # Clean tweets\n",
    "    from tqdm.auto import tqdm\n",
    "    tqdm.pandas()\n",
    "\n",
    "    df['tokens'] = df['text'].progress_apply(clean_tweet)\n",
    "    df['clean_text'] = df['tokens'].apply(lambda toks: \" \".join(toks))\n",
    "\n",
    "    # Map classes to labels\n",
    "    label_map = {\n",
    "        0: \"hate_speech\",\n",
    "        1: \"offensive_language\",\n",
    "        2: \"neither\"\n",
    "    }\n",
    "    df['label_name'] = df['class'].map(label_map)\n",
    "\n",
    "    # numeric label column (0/1/2) used by model\n",
    "    df['label'] = df['class'].astype(int)\n",
    "\n",
    "    # rename tweet -> text (our pipeline expects 'text')\n",
    "    df = df.rename(columns={'tweet': 'text'})\n",
    "\n",
    "    # Show summary\n",
    "    print(\"Loaded:\", filename)\n",
    "    print(\"Label distribution (counts):\")\n",
    "    print(df['label'].value_counts())\n",
    "    print(df.head(3))\n",
    "\n",
    "    # Now split: train / temp(test) then split train->train+val\n",
    "    train_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=test_size,\n",
    "        stratify=df['label'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Now carve out val from the train_df (val_size fraction of original train)\n",
    "    # compute val fraction relative to train_df:\n",
    "    rel_val = val_size / (1.0 - test_size)\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_df,\n",
    "        test_size=rel_val,\n",
    "        stratify=train_df['label'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    print(f\"Split sizes: Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "    print(\"\\nTrain label distribution:\")\n",
    "    print(train_df['label'].value_counts())\n",
    "\n",
    "    # also produce lists (used by tokenizer/dataset code)\n",
    "    train_texts, train_labels = train_df['clean_text'].tolist(), train_df['label'].tolist()\n",
    "    val_texts, val_labels = val_df['clean_text'].tolist(), val_df['label'].tolist()\n",
    "    test_texts, test_labels = test_df['clean_text'].tolist(), test_df['label'].tolist()\n",
    "\n",
    "    return train_df, val_df, test_df, (train_texts, train_labels, val_texts, val_labels, test_texts, test_labels)\n",
    "\n",
    "file_path = \"data/labeled_data.csv\"\n",
    "train_df, val_df, test_df, (train_texts, train_labels, val_texts, val_labels, test_texts, test_labels) = read_data_df(file_path)\n",
    "\n",
    "sns.countplot(x=train_df['label'])\n",
    "plt.title(\"Training Label Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "66627107-bcf1-4f66-bdc8-0584b040b1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders ready: train/val/test sizes: 17347 2479 4957\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer + Dataset + Dataloaders\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class HFTextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=MAX_LENGTH):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        # if there's an existing tokenize_texts function, use it (expects a single string)\n",
    "        if 'tokenize_texts' in globals():\n",
    "            enc = tokenize_texts(text, tokenizer=tokenizer, max_length=self.max_length)\n",
    "            # tokenize_texts should return dict with input_ids, attention_mask\n",
    "        elif 'tokenize' in globals():\n",
    "            enc = tokenize(text, tokenizer=tokenizer, max_length=self.max_length)\n",
    "        else:\n",
    "            enc = tokenizer(\n",
    "                text,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "            )\n",
    "        enc_input_ids = torch.tensor(enc['input_ids'], dtype=torch.long)\n",
    "        enc_attention_mask = torch.tensor(enc.get('attention_mask', [1]*len(enc['input_ids'])), dtype=torch.long)\n",
    "        label = torch.tensor(int(self.labels[idx]), dtype=torch.long)\n",
    "        return {'input_ids': enc_input_ids, 'attention_mask': enc_attention_mask, 'labels': label}\n",
    "\n",
    "# Collate fn: use tokenizer.pad for dynamic padding (safer)\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    batch_enc = tokenizer.pad(\n",
    "        {'input_ids': input_ids, 'attention_mask': attention_mask},\n",
    "        padding='longest',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    batch_enc['labels'] = labels\n",
    "    return batch_enc\n",
    "\n",
    "# create datasets\n",
    "train_dataset = HFTextDataset(train_texts, train_labels, tokenizer, max_length=MAX_LENGTH)\n",
    "val_dataset = HFTextDataset(val_texts, val_labels, tokenizer, max_length=MAX_LENGTH)\n",
    "test_dataset = HFTextDataset(test_texts, test_labels, tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"Dataloaders ready: train/val/test sizes:\", len(train_dataset), len(val_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6533e47b-fbc0-4162-a2f7-da9d43770eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num labels: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a30cf95f774481845f8c74f351ad87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train E1:   0%|          | 0/1085 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     48\u001b[39m     scaler.update()\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     loss = outputs.loss\n\u001b[32m     52\u001b[39m     loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/transformers/models/roberta/modeling_roberta.py:1188\u001b[39m, in \u001b[36mRobertaForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1171\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1172\u001b[39m \u001b[33;03mtoken_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1173\u001b[39m \u001b[33;03m    Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1184\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1185\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1186\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1188\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1199\u001b[39m sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1200\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.classifier(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/transformers/models/roberta/modeling_roberta.py:862\u001b[39m, in \u001b[36mRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    855\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    856\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    857\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    858\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    859\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    860\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    875\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    876\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/transformers/models/roberta/modeling_roberta.py:606\u001b[39m, in \u001b[36mRobertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    602\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m    604\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    617\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/transformers/models/roberta/modeling_roberta.py:543\u001b[39m, in \u001b[36mRobertaLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    540\u001b[39m     attention_output = cross_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    541\u001b[39m     outputs = outputs + cross_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    546\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/transformers/pytorch_utils.py:257\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    254\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/transformers/models/roberta/modeling_roberta.py:552\u001b[39m, in \u001b[36mRobertaLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m    551\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m     layer_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/transformers/models/roberta/modeling_roberta.py:479\u001b[39m, in \u001b[36mRobertaOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    481\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/CSC396/threat-detection-sim/venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Model + training loop\n",
    "num_labels = len(sorted(set(train_labels)))\n",
    "print(\"Num labels:\", num_labels)\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=num_labels, finetuning_task=\"classification\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "model.to(device)\n",
    "\n",
    "# optimizer + scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "warmup_steps = max(1, int(0.06 * total_steps))\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# Optionally enable mixed precision if available (requires accelerate or torch.cuda.amp)\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "\n",
    "def save_checkpoint(epoch):\n",
    "    path = OUTPUT_DIR / f\"roberta_epoch{epoch}.pt\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state': model.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict(),\n",
    "        'scheduler_state': scheduler.state_dict(),\n",
    "    }, path)\n",
    "    print(\"Saved checkpoint:\", path)\n",
    "\n",
    "# training\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_f1': []}\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model.train()\n",
    "    t_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Train E{epoch}\", leave=True)\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        if use_amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        t_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_train_loss = t_loss / len(train_loader)\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch} â€” avg train loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    v_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels_eval = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            v_loss += outputs.loss.item()\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels_eval.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    avg_val_loss = v_loss / len(val_loader)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels_eval, all_preds, average='macro', zero_division=0)\n",
    "    history['val_f1'].append(f1)\n",
    "    print(f\"Epoch {epoch} â€” val_loss: {avg_val_loss:.4f} â€” val_macro_f1: {f1:.4f}\")\n",
    "\n",
    "    save_checkpoint(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9561dc-caa7-4ae8-8231-83573f5c0dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation + report + confusion matrix\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels_eval = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_labels_eval.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "print(classification_report(all_labels_eval, all_preds, digits=4, zero_division=0, target_names=[str(i) for i in range(num_labels)]))\n",
    "\n",
    "cm = confusion_matrix(all_labels_eval, all_preds)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[str(i) for i in range(num_labels)], yticklabels=[str(i) for i in range(num_labels)])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion matrix (test)')\n",
    "plt.show()\n",
    "\n",
    "if 'label_map' in globals():\n",
    "    target_names = [label_map[i] for i in sorted(label_map.keys())]\n",
    "    print(classification_report(all_labels_eval, all_preds, digits=4, zero_division=0, target_names=target_names))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (threat-env)",
   "language": "python",
   "name": "threat-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
